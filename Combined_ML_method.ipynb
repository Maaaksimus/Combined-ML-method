{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Установка библиотеки RDKit"
      ],
      "metadata": {
        "id": "zHUMsGAigW8T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T-210TzqvKPN"
      },
      "outputs": [],
      "source": [
        "pip -q install rdkit-pypi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSl1flY3py5A"
      },
      "source": [
        "# Подключение библиотек"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dlKO6Xx2py5A"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "\n",
        "import warnings\n",
        "\n",
        "from rdkit import Chem\n",
        "from rdkit import RDLogger\n",
        "from rdkit.Chem.Draw import IPythonConsole\n",
        "from rdkit.Chem.Draw import MolsToGridImage\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "RDLogger.DisableLog(\"rdApp.*\")\n",
        "\n",
        "np.random.seed(29)\n",
        "tf.random.set_seed(29)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKDNIKgnpy5C"
      },
      "source": [
        "# Загрузка данных"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C9t1IkWfyMuC"
      },
      "outputs": [],
      "source": [
        "csv_path = keras.utils.get_file(\n",
        "    \"Lipophilicity.csv\", \"https://deepchemdata.s3-us-west-1.amazonaws.com/datasets/Lipophilicity.csv\"\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zbn5tjBZpy5D"
      },
      "source": [
        "### Извлечение признаков"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RlRAFuQOpy5D"
      },
      "outputs": [],
      "source": [
        " class FeatureExtraction:\n",
        "    def __init__(self, allowable_sets):\n",
        "        self.dim = 0\n",
        "        self.features_mapping = {}\n",
        "        for k, s in allowable_sets.items():\n",
        "            s = sorted(list(s))\n",
        "            self.features_mapping[k] = dict(zip(s, range(self.dim, len(s) + self.dim)))\n",
        "            self.dim += len(s)\n",
        "\n",
        "    def encode(self, inputs):\n",
        "        output = np.zeros((self.dim,))\n",
        "        for name_feature, feature_mapping in self.features_mapping.items():\n",
        "            feature = getattr(self, name_feature)(inputs)\n",
        "            if feature not in feature_mapping:\n",
        "                continue\n",
        "            output[feature_mapping[feature]] = 1.0\n",
        "        return output\n",
        "\n",
        "\n",
        "class AtomFeatureExtraction(FeatureExtraction):\n",
        "    def __init__(self, allowable_sets):\n",
        "        super().__init__(allowable_sets)\n",
        "\n",
        "    def symbol(self, atom):\n",
        "        return atom.GetSymbol()\n",
        "\n",
        "    def n_valence(self, atom):\n",
        "        return atom.GetTotalValence()\n",
        "\n",
        "    def n_hydrogens(self, atom):\n",
        "        return atom.GetTotalNumHs()\n",
        "\n",
        "    def hybridization(self, atom):\n",
        "        return atom.GetHybridization().name.lower()\n",
        "\n",
        "\n",
        "class BondFeatureExtraction(FeatureExtraction):\n",
        "    def __init__(self, allowable_sets):\n",
        "        super().__init__(allowable_sets)\n",
        "        self.dim += 1\n",
        "\n",
        "    def encode(self, bond):\n",
        "        output = np.zeros((self.dim,))\n",
        "        if bond is None:\n",
        "            output[-1] = 1.0\n",
        "            return output\n",
        "        output = super().encode(bond)\n",
        "        return output\n",
        "\n",
        "    def bond_type(self, bond):\n",
        "        return bond.GetBondType().name.lower()\n",
        "\n",
        "    def conjugated(self, bond):\n",
        "        return bond.GetIsConjugated()\n",
        "\n",
        "\n",
        "atom_extracter = AtomFeatureExtraction(\n",
        "    allowable_sets={\n",
        "        \"symbol\": {\"B\", \"Br\", \"C\", \"Ca\", \"Cl\", \"F\", \"H\", \"I\", \"N\", \"Na\", \"O\", \"P\", \"S\"},\n",
        "        \"n_valence\": {0, 1, 2, 3, 4, 5, 6},\n",
        "        \"n_hydrogens\": {0, 1, 2, 3, 4},\n",
        "        \"hybridization\": {\"s\", \"sp\", \"sp2\", \"sp3\"},\n",
        "    }\n",
        ")\n",
        "\n",
        "bond_extracter = BondFeatureExtraction(\n",
        "    allowable_sets={\n",
        "        \"bond_type\": {\"single\", \"double\", \"triple\", \"aromatic\"},\n",
        "        \"conjugated\": {True, False},\n",
        "    }\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hh9yQuPZpy5E"
      },
      "source": [
        "### Создание трёхмерного тензора для описания молекулярного графа"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zlaeaEh5py5F"
      },
      "outputs": [],
      "source": [
        "\n",
        "def molecule_creator(smiles):\n",
        "    molecule = Chem.MolFromSmiles(smiles, sanitize=False)\n",
        "\n",
        "    flag = Chem.SanitizeMol(molecule, catchErrors=True)\n",
        "    if flag != Chem.SanitizeFlags.SANITIZE_NONE:\n",
        "        Chem.SanitizeMol(molecule, sanitizeOps=Chem.SanitizeFlags.SANITIZE_ALL ^ flag)\n",
        "\n",
        "    Chem.AssignStereochemistry(molecule, cleanIt=True, force=True)\n",
        "    return molecule\n",
        "\n",
        "\n",
        "def graph_creator(molecule):\n",
        "\n",
        "    atom_features = []\n",
        "    bond_features = []\n",
        "    pair_indices = []\n",
        "\n",
        "    for atom in molecule.GetAtoms():\n",
        "        atom_features.append(atom_extracter.encode(atom))\n",
        "\n",
        "        pair_indices.append([atom.GetIdx(), atom.GetIdx()])\n",
        "        bond_features.append(bond_extracter.encode(None))\n",
        "\n",
        "        for neighbor in atom.GetNeighbors():\n",
        "            bond = molecule.GetBondBetweenAtoms(atom.GetIdx(), neighbor.GetIdx())\n",
        "            pair_indices.append([atom.GetIdx(), neighbor.GetIdx()])\n",
        "            bond_features.append(bond_extracter.encode(bond))\n",
        "\n",
        "    return np.array(atom_features), np.array(bond_features), np.array(pair_indices)\n",
        "\n",
        "\n",
        "def get_graphs(smiles_list):\n",
        "\n",
        "    atom_features_list = []\n",
        "    bond_features_list = []\n",
        "    pair_indices_list = []\n",
        "\n",
        "    for smiles in smiles_list:\n",
        "        molecule = molecule_creator(smiles)\n",
        "        atom_features, bond_features, pair_indices = graph_creator(molecule)\n",
        "\n",
        "        atom_features_list.append(atom_features)\n",
        "        bond_features_list.append(bond_features)\n",
        "        pair_indices_list.append(pair_indices)\n",
        "\n",
        "    return (\n",
        "        tf.ragged.constant(atom_features_list, dtype=tf.float32),\n",
        "        tf.ragged.constant(bond_features_list, dtype=tf.float32),\n",
        "        tf.ragged.constant(pair_indices_list, dtype=tf.int64),\n",
        "    )\n",
        "\n",
        "\n",
        "permuted_indices = np.random.permutation(np.arange(df.shape[0]))\n",
        "\n",
        "# Подвыборка для тренировки: 80 % данных\n",
        "train_index = permuted_indices[: int(df.shape[0] * 0.8)]\n",
        "x_train = get_graphs(df.iloc[train_index].smiles)\n",
        "y_train = df.iloc[train_index].exp\n",
        "\n",
        "# Подвыборка для валидации: 20 % данных\n",
        "valid_index = permuted_indices[int(df.shape[0] * 0.8) : int(df.shape[0])]\n",
        "x_valid = get_graphs(df.iloc[valid_index].smiles)\n",
        "y_valid = df.iloc[valid_index].exp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MN6rEnABpy5H"
      },
      "source": [
        "### Создание датасета"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xWPdxVKYpy5H"
      },
      "outputs": [],
      "source": [
        "def get_batch(x_batch, y_batch):\n",
        "\n",
        "    atom_features, bond_features, pair_indices = x_batch\n",
        "\n",
        "    num_atoms = atom_features.row_lengths()\n",
        "    num_bonds = bond_features.row_lengths()\n",
        "\n",
        "    molecule_indices = tf.range(len(num_atoms))\n",
        "    molecule_indicator = tf.repeat(molecule_indices, num_atoms)\n",
        "\n",
        "    gather_indices = tf.repeat(molecule_indices[:-1], num_bonds[1:])\n",
        "\n",
        "    increment = tf.cumsum(num_atoms[:-1])\n",
        "    increment = tf.pad(tf.gather(increment, gather_indices), [(num_bonds[0], 0)])\n",
        "\n",
        "    pair_indices = pair_indices.merge_dims(outer_axis=0, inner_axis=1).to_tensor()\n",
        "    pair_indices = pair_indices + increment[:, tf.newaxis]\n",
        "    atom_features = atom_features.merge_dims(outer_axis=0, inner_axis=1).to_tensor()\n",
        "    bond_features = bond_features.merge_dims(outer_axis=0, inner_axis=1).to_tensor()\n",
        "\n",
        "    return (atom_features, bond_features, pair_indices, molecule_indicator), y_batch\n",
        "\n",
        "\n",
        "def Dataset(X, y, batch_size=32, shuffle=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, (y)))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(1024)\n",
        "    return dataset.batch(batch_size).map(get_batch, -1).prefetch(-1)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fq5jbCrHpy5I"
      },
      "source": [
        "# Модель"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Механизм Message passing"
      ],
      "metadata": {
        "id": "yujxqjttebch"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZyEMFD7py5J"
      },
      "outputs": [],
      "source": [
        "class NeighborsMessage(layers.Layer):\n",
        "    def build(self, input_shape):\n",
        "        self.atom_dim = input_shape[0][-1]\n",
        "        self.bond_dim = input_shape[1][-1]\n",
        "        self.kernel = self.add_weight(\n",
        "            shape=(self.bond_dim, self.atom_dim * self.atom_dim),\n",
        "            initializer=\"glorot_uniform\",\n",
        "            name=\"kernel\",\n",
        "        )\n",
        "        self.bias = self.add_weight(\n",
        "            shape=(self.atom_dim * self.atom_dim), initializer=\"zeros\", name=\"bias\", # создание bias вектора\n",
        "        )\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        atom_features, bond_features, pair_indices = inputs\n",
        "\n",
        "        bond_features = tf.matmul(bond_features, self.kernel) + self.bias\n",
        "\n",
        "        bond_features = tf.reshape(bond_features, (-1, self.atom_dim, self.atom_dim))\n",
        "\n",
        "        neighbors_features = tf.gather(atom_features, pair_indices[:, 1])\n",
        "        neighbors_features = tf.expand_dims(neighbors_features, axis=-1)\n",
        "\n",
        "        weighted_features = tf.matmul(bond_features, neighbors_features)\n",
        "        weighted_features = tf.squeeze(weighted_features, axis=-1)\n",
        "        aggregated_features = tf.math.unsorted_segment_sum(\n",
        "            weighted_features,\n",
        "            pair_indices[:, 0],\n",
        "            num_segments=tf.shape(atom_features)[0],\n",
        "        )\n",
        "        return aggregated_features\n",
        "\n",
        "\n",
        "#\n",
        "class MessagePassing(layers.Layer):\n",
        "    def __init__(self, units, steps=4, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.units = units\n",
        "        self.steps = steps\n",
        "\n",
        "    def build(self, input_shape):\n",
        "        self.atom_dim = input_shape[0][-1]\n",
        "        self.message_step = NeighborsMessage()\n",
        "        self.pad_length = max(0, self.units - self.atom_dim)\n",
        "        self.update_step = layers.GRUCell(self.atom_dim + self.pad_length)\n",
        "        self.built = True\n",
        "\n",
        "    def call(self, inputs):\n",
        "        atom_features, bond_features, pair_indices = inputs\n",
        "\n",
        "        atom_features_updated = tf.pad(atom_features, [(0, 0), (0, self.pad_length)])\n",
        "\n",
        "        for i in range(self.steps):\n",
        "            atom_features_aggregated = self.message_step(\n",
        "                [atom_features_updated, bond_features, pair_indices]\n",
        "            )\n",
        "\n",
        "            atom_features_updated, _ = self.update_step(\n",
        "                atom_features_aggregated, atom_features_updated\n",
        "            )\n",
        "        return atom_features_updated\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0KETyS7bpy5J"
      },
      "source": [
        "## Обобщающая фукнция"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TYt-JKBspy5K"
      },
      "outputs": [],
      "source": [
        "\n",
        "class AdderPaddind(layers.Layer):\n",
        "    def __init__(self, batch_size, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "    def call(self, inputs):\n",
        "\n",
        "        atom_features, molecule_indicator = inputs\n",
        "\n",
        "        atom_features_partitioned = tf.dynamic_partition(\n",
        "            atom_features, molecule_indicator, self.batch_size\n",
        "        )\n",
        "\n",
        "        num_atoms = [tf.shape(f)[0] for f in atom_features_partitioned]\n",
        "        max_num_atoms = tf.reduce_max(num_atoms)\n",
        "        atom_features_stacked = tf.stack(\n",
        "            [\n",
        "                tf.pad(f, [(0, max_num_atoms - n), (0, 0)])\n",
        "                for f, n in zip(atom_features_partitioned, num_atoms)\n",
        "            ],\n",
        "            axis=0,\n",
        "        )\n",
        "\n",
        "        gather_indices = tf.where(tf.reduce_sum(atom_features_stacked, (1, 2)) != 0)\n",
        "        gather_indices = tf.squeeze(gather_indices, axis=-1)\n",
        "        return tf.gather(atom_features_stacked, gather_indices, axis=0)\n",
        "\n",
        "\n",
        "class ReadoutWithAttention(layers.Layer):\n",
        "    def __init__(\n",
        "        self, num_heads=8, embed_dim=64, dense_dim=512, batch_size=32, **kwargs\n",
        "    ):\n",
        "        super().__init__(**kwargs)\n",
        "\n",
        "        self.partition_padding = AdderPaddind(batch_size)\n",
        "        self.attention = layers.MultiHeadAttention(num_heads, embed_dim)\n",
        "        self.dense_proj = keras.Sequential(\n",
        "            [layers.Dense(dense_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n",
        "        )\n",
        "        self.layernorm_1 = layers.LayerNormalization()\n",
        "        self.layernorm_2 = layers.LayerNormalization()\n",
        "        self.average_pooling = layers.GlobalAveragePooling1D()\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.partition_padding(inputs)\n",
        "        padding_mask = tf.reduce_any(tf.not_equal(x, 0.0), axis=-1)\n",
        "        padding_mask = padding_mask[:, tf.newaxis, tf.newaxis, :]\n",
        "        attention_output = self.attention(x, x, attention_mask=padding_mask)\n",
        "        proj_input = self.layernorm_1(x + attention_output)\n",
        "        proj_output = self.layernorm_2(proj_input + self.dense_proj(proj_input))\n",
        "        return self.average_pooling(proj_output)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px6KfTkOpy5K"
      },
      "source": [
        "# Смешанная модель машинного обучения"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4wGenObSaib"
      },
      "outputs": [],
      "source": [
        "\n",
        "def GNNmodel(\n",
        "    atom_dim,\n",
        "    bond_dim,\n",
        "    batch_size=32,\n",
        "    message_units=64,\n",
        "    message_steps=4,\n",
        "    num_attention_heads=8,\n",
        "    dense_units=256,\n",
        "):\n",
        "\n",
        "    atom_features = layers.Input((atom_dim), dtype=\"float32\", name=\"atom_features\")\n",
        "    bond_features = layers.Input((bond_dim), dtype=\"float32\", name=\"bond_features\")\n",
        "    pair_indices = layers.Input((2), dtype=\"int32\", name=\"pair_indices\")\n",
        "    molecule_indicator = layers.Input((), dtype=\"int32\", name=\"molecule_indicator\")\n",
        "\n",
        "    mp = MessagePassing(message_units, message_steps)(\n",
        "        [atom_features, bond_features, pair_indices]\n",
        "    )\n",
        "\n",
        "    transf = ReadoutWithAttention(\n",
        "        num_attention_heads, message_units, dense_units, batch_size\n",
        "    )([mp, molecule_indicator])\n",
        "\n",
        "    l1 = layers.Dense(dense_units, activation=\"relu\")(transf)\n",
        "    l2 = layers.Dense(1, activation=\"linear\")(l1)\n",
        "\n",
        "    full_model = keras.Model(\n",
        "        inputs=[atom_features, bond_features, pair_indices, molecule_indicator],\n",
        "        outputs=[l2]\n",
        "    )\n",
        "    clust_model = keras.Model(\n",
        "        [atom_features, bond_features, pair_indices, molecule_indicator],\n",
        "        l1\n",
        "    )\n",
        "\n",
        "    return full_model, clust_model\n",
        "\n",
        "mpnn_full, mpnn_clust = GNNmodel(\n",
        "    atom_dim=x_train[0][0][0].shape[0], bond_dim=x_train[1][0][0].shape[0],\n",
        ")\n",
        "\n",
        "mpnn_full.compile(\n",
        "    loss=keras.losses.MeanSquaredError(),\n",
        "    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "    metrics=[keras.metrics.MeanAbsoluteError(name=\"MAE\")],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtMIeUMepy5L"
      },
      "source": [
        "## Обучение"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y-zyQqlgpy5L"
      },
      "outputs": [],
      "source": [
        "train_dataset = Dataset(x_train, y_train)\n",
        "valid_dataset = Dataset(x_valid, y_valid)\n",
        "\n",
        "history = mpnn_full.fit(\n",
        "    train_dataset,\n",
        "    validation_data=valid_dataset,\n",
        "    epochs=50,\n",
        "    verbose=2\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "min(history.history['val_MAE'])"
      ],
      "metadata": {
        "id": "tUdQtuQr7_vK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IJm3ibUjGu9O"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(history.history[\"MAE\"], label=\"train MAE\")\n",
        "plt.plot(history.history[\"val_MAE\"], label=\"valid MAE\")\n",
        "plt.xlabel(\"Эпохи\", fontsize=16)\n",
        "plt.ylabel(\"MAE\", fontsize=16)\n",
        "plt.legend(fontsize=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUC636eKufLN"
      },
      "source": [
        "## Кластеризация"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v4mx5O-QulZj"
      },
      "outputs": [],
      "source": [
        "from sklearn.cluster import KMeans, AgglomerativeClustering\n",
        "from sklearn import metrics\n",
        "from scipy.spatial.distance import cdist"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Методы"
      ],
      "metadata": {
        "id": "vlXsr8SP05Le"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7D8csnABQbip"
      },
      "outputs": [],
      "source": [
        "def cluster_sets(n, data_inp, data_targ, clust_index):\n",
        "  clusters = []\n",
        "  targets = []\n",
        "  for i in range(n):\n",
        "    clust = data_inp[clust_index == i]\n",
        "    targ = data_targ[clust_index == i]\n",
        "    clusters.append(np.array(clust))\n",
        "    targets.append(np.array(targ))\n",
        "\n",
        "  return np.array(clusters), np.array(targets)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YP3hgCg72osr"
      },
      "outputs": [],
      "source": [
        "def VecDataset(X, y, batch_size=32, shuffle=False):\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((X, (y)))\n",
        "    if shuffle:\n",
        "        dataset = dataset.shuffle(1024)\n",
        "    return dataset.batch(batch_size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gaJP6bRN0cup"
      },
      "outputs": [],
      "source": [
        "def models_fiting(n, clusted_data_train, clusted_data_valid, clusted_target_train, clusted_target_valid):\n",
        "\n",
        "  models = []\n",
        "\n",
        "  for i in range(n):\n",
        "    inp = layers.Input(256, dtype=\"float32\")\n",
        "    out = layers.Dense(1, activation=\"linear\")(inp)\n",
        "    new_model = keras.Model(inp, out)\n",
        "\n",
        "    new_model.compile(\n",
        "      loss=keras.losses.MeanSquaredError(),\n",
        "      optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n",
        "      metrics=[keras.metrics.MeanAbsoluteError(name=\"MAE\")],\n",
        "    )\n",
        "\n",
        "    train = VecDataset(clusted_data_train[i], clusted_target_train[i])\n",
        "    valid = VecDataset(clusted_data_valid[i], clusted_target_valid[i])\n",
        "\n",
        "    models.append(new_model.fit(\n",
        "        train,\n",
        "        validation_data=valid,\n",
        "        epochs=50,\n",
        "        verbose=0\n",
        "    ))\n",
        "\n",
        "  return models\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tr = mpnn_clust.predict(train_dataset)"
      ],
      "metadata": {
        "id": "x7nKlbQAkc-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "NUM_OF_CLUSTERS = 6"
      ],
      "metadata": {
        "id": "wjT2nwja1Ncx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qRjgoHnA5eIo"
      },
      "outputs": [],
      "source": [
        "knn = KMeans(n_clusters=NUM_OF_CLUSTERS).fit(tr)\n",
        "hw = AgglomerativeClustering(n_clusters=NUM_OF_CLUSTERS, linkage='ward').fit(tr)\n",
        "ha = AgglomerativeClustering(n_clusters=NUM_OF_CLUSTERS, linkage='average').fit(tr)\n",
        "hc = AgglomerativeClustering(n_clusters=NUM_OF_CLUSTERS, linkage='complete').fit(tr)\n",
        "hs = AgglomerativeClustering(n_clusters=NUM_OF_CLUSTERS, linkage='single').fit(tr)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BApZlV3mYgwG"
      },
      "outputs": [],
      "source": [
        "train_ds = mpnn_clust.predict(train_dataset)\n",
        "valid_ds = mpnn_clust.predict(valid_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "clusted_train_ds, clusted_y_train = [], []\n",
        "clusted_valid_ds, clusted_y_valid = [], []"
      ],
      "metadata": {
        "id": "Y4blicht1zWL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "0zPNnoDqlEFG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A-Na_LweZklM"
      },
      "outputs": [],
      "source": [
        "clusted_train_ds_buf, clusted_y_train_buf = cluster_sets(NUM_OF_CLUSTERS, train_ds, y_train, knn.predict(train_ds))\n",
        "clusted_valid_ds_buf, clusted_y_valid_buf = cluster_sets(NUM_OF_CLUSTERS, valid_ds, y_valid, knn.predict(valid_ds))\n",
        "\n",
        "clusted_train_ds.append(clusted_train_ds_buf)\n",
        "clusted_y_train.append(clusted_y_train_buf)\n",
        "clusted_valid_ds.append(clusted_valid_ds_buf)\n",
        "clusted_y_valid.append(clusted_y_valid_buf)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ward"
      ],
      "metadata": {
        "id": "Z_1x9eqclF8i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusted_train_ds_buf, clusted_y_train_buf = cluster_sets(NUM_OF_CLUSTERS, train_ds, y_train, hw.fit_predict(train_ds))\n",
        "clusted_valid_ds_buf, clusted_y_valid_buf = cluster_sets(NUM_OF_CLUSTERS, valid_ds, y_valid, hw.fit_predict(valid_ds))\n",
        "\n",
        "clusted_train_ds.append(clusted_train_ds_buf)\n",
        "clusted_y_train.append(clusted_y_train_buf)\n",
        "clusted_valid_ds.append(clusted_valid_ds_buf)\n",
        "clusted_y_valid.append(clusted_y_valid_buf)"
      ],
      "metadata": {
        "id": "n27grN8wlCaM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Average"
      ],
      "metadata": {
        "id": "Mq0xhgD03Nar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusted_train_ds_buf, clusted_y_train_buf = cluster_sets(NUM_OF_CLUSTERS, train_ds, y_train, ha.fit_predict(train_ds))\n",
        "clusted_valid_ds_buf, clusted_y_valid_buf = cluster_sets(NUM_OF_CLUSTERS, valid_ds, y_valid, ha.fit_predict(valid_ds))\n",
        "\n",
        "clusted_train_ds.append(clusted_train_ds_buf)\n",
        "clusted_y_train.append(clusted_y_train_buf)\n",
        "clusted_valid_ds.append(clusted_valid_ds_buf)\n",
        "clusted_y_valid.append(clusted_y_valid_buf)"
      ],
      "metadata": {
        "id": "qHGexeX52_sP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Complete"
      ],
      "metadata": {
        "id": "CWxPdNZp3Rmf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusted_train_ds_buf, clusted_y_train_buf = cluster_sets(NUM_OF_CLUSTERS, train_ds, y_train, hc.fit_predict(train_ds))\n",
        "clusted_valid_ds_buf, clusted_y_valid_buf = cluster_sets(NUM_OF_CLUSTERS, valid_ds, y_valid, hc.fit_predict(valid_ds))\n",
        "\n",
        "clusted_train_ds.append(clusted_train_ds_buf)\n",
        "clusted_y_train.append(clusted_y_train_buf)\n",
        "clusted_valid_ds.append(clusted_valid_ds_buf)\n",
        "clusted_y_valid.append(clusted_y_valid_buf)"
      ],
      "metadata": {
        "id": "8bKHJL_y2_af"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Single"
      ],
      "metadata": {
        "id": "N8IKXrmD3TV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "clusted_train_ds_buf, clusted_y_train_buf = cluster_sets(NUM_OF_CLUSTERS, train_ds, y_train, hs.fit_predict(train_ds))\n",
        "clusted_valid_ds_buf, clusted_y_valid_buf = cluster_sets(NUM_OF_CLUSTERS, valid_ds, y_valid, hs.fit_predict(valid_ds))\n",
        "\n",
        "clusted_train_ds.append(clusted_train_ds_buf)\n",
        "clusted_y_train.append(clusted_y_train_buf)\n",
        "clusted_valid_ds.append(clusted_valid_ds_buf)\n",
        "clusted_y_valid.append(clusted_y_valid_buf)"
      ],
      "metadata": {
        "id": "3_EiaDtY2-mg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for clust_method in clusted_valid_ds:\n",
        "  clusters_size = [len(cl) for cl in clust_method]\n",
        "  print(clusters_size)"
      ],
      "metadata": {
        "id": "Jcg1YTU131SS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "KNN"
      ],
      "metadata": {
        "id": "8AVAAeK4ljAP"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3o8o4fPxYDXb"
      },
      "outputs": [],
      "source": [
        "mod_knn = models_fiting(NUM_OF_CLUSTERS, clusted_train_ds[0], clusted_valid_ds[0], clusted_y_train[0], clusted_y_valid[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ward"
      ],
      "metadata": {
        "id": "4VkNxxmdlkuR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_hw = models_fiting(NUM_OF_CLUSTERS, clusted_train_ds[1], clusted_valid_ds[1], clusted_y_train[1], clusted_y_valid[1])"
      ],
      "metadata": {
        "id": "DApKijl2lhUr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "average"
      ],
      "metadata": {
        "id": "bwfRsYm_Gyov"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_ha = models_fiting(NUM_OF_CLUSTERS, clusted_train_ds[2], clusted_valid_ds[2], clusted_y_train[2], clusted_y_valid[2])"
      ],
      "metadata": {
        "id": "b6mIB9Ui5iv8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "complete"
      ],
      "metadata": {
        "id": "JOqlOdZgfsad"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_hc = models_fiting(NUM_OF_CLUSTERS, clusted_train_ds[3], clusted_valid_ds[3], clusted_y_train[3], clusted_y_valid[3])"
      ],
      "metadata": {
        "id": "LEChcj2l5ig4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "single"
      ],
      "metadata": {
        "id": "QW23wtIVG1yY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mod_hs = models_fiting(NUM_OF_CLUSTERS, clusted_train_ds[4], clusted_valid_ds[4], clusted_y_train[4], clusted_y_valid[4])"
      ],
      "metadata": {
        "id": "TTETCCD45iUZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"KNN:\")\n",
        "for i in range(NUM_OF_CLUSTERS):\n",
        "  print(min(mod_knn[i].history['val_MAE']))\n",
        "\n",
        "print(\"HW:\")\n",
        "for i in range(NUM_OF_CLUSTERS):\n",
        "  print(min(mod_hw[i].history['val_MAE']))\n",
        "\n",
        "print(\"HA:\")\n",
        "for i in range(NUM_OF_CLUSTERS):\n",
        "  print(min(mod_ha[i].history['val_MAE']))\n",
        "\n",
        "print(\"HC:\")\n",
        "for i in range(NUM_OF_CLUSTERS):\n",
        "  print(min(mod_hc[i].history['val_MAE']))\n",
        "\n",
        "print(\"HS:\")\n",
        "for i in range(NUM_OF_CLUSTERS):\n",
        "  print(min(mod_hs[i].history['val_MAE']))"
      ],
      "metadata": {
        "id": "CF9fLWXM6O6P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Метод локтя"
      ],
      "metadata": {
        "id": "3Cg-B4fhzm-q"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uJIsZvaiu5qz"
      },
      "outputs": [],
      "source": [
        "X = mpnn_clust.predict(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GHbTWSiAuwZE"
      },
      "outputs": [],
      "source": [
        "distortions = []\n",
        "inertias = []\n",
        "mapping1 = {}\n",
        "mapping2 = {}\n",
        "K = range(1, 10)\n",
        "\n",
        "for k in K:\n",
        "\tkmeanModel = KMeans(n_clusters=k).fit(X)\n",
        "\tkmeanModel.fit(X)\n",
        "\n",
        "\tdistortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
        "\t\t\t\t\t\t\t\t\t\t'euclidean'), axis=1)) / X.shape[0])\n",
        "\tinertias.append(kmeanModel.inertia_)\n",
        "\n",
        "\tmapping1[k] = sum(np.min(cdist(X, kmeanModel.cluster_centers_,\n",
        "\t\t\t\t\t\t\t\t'euclidean'), axis=1)) / X.shape[0]\n",
        "\tmapping2[k] = kmeanModel.inertia_\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jO2dlxdbuy-K"
      },
      "outputs": [],
      "source": [
        "for key, val in mapping1.items():\n",
        "\tprint(f'{key} : {val}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "THpC0ob_vG0k"
      },
      "outputs": [],
      "source": [
        "plt.plot(K, distortions, 'bx-')\n",
        "plt.xlabel('Значение K')\n",
        "plt.ylabel('Отклонение')\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "zHUMsGAigW8T"
      ],
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.0"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}